---
layout:     post
title:      本地部署DeepSeek并集成硅基流动
subtitle:   
date:       2025-02-06
author:     dm
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - AI





---

## 一、本地安装Deepseek R1

`Ollama`本地安装`Deepseek R1`模型

打开 [Ollama官网地址](https://ollama.com/)，然后点击下载，之后安装到本地。

![图片](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/OllamaDownload.png)

然后打开`Ollama`的模型列表，搜索到`DeepSeek R1`

[deepseek-r1模型下载地址](https://ollama.com/library/deepseek-r1)

![Ollmam模型选择](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/Ollmam模型选择.png)

模型对电脑配置要求较高`1.5b`或`7b`【**右上角的的代码**】复制在命令行中运行。**本人电脑16G内存选择7b**。

---

![图片](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/下载模型.png)

安装需要一段时间，我们等一下就可以等`success`，就代表安装成功。 

输入`【ollama list】`，就可以查看安装的模型**。**

---

![图片](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/查看模型list.png)

设置安装之后，我们只能在命令行中使用会特别的不方便。

我们需要找到一个第三方客户端。

客户端推荐`Chatbox`和`Cherry Studio`，都很优秀。

---



## 二、Cherry Studio安装

[Cherry Studio 下载地址](https://cherry-ai.com/)

`Cherry Studio`是一个特别强大的AI 客户端，支持国内外很多模型。

还内置很多提示词，文生图，文档等功能。

按照下面步骤添加即可。

> 我们在Cherry Studio 客户端配置Ollama 安装过的模型。

默认API：**http://localhost:11434/v1**

模型名：**deepseek-r1:1.5b**

![图片](https://mmbiz.qpic.cn/mmbiz_png/ib9Wg6icQ0Q2R6yC5zFbPwZgLjHAX0Ec1MD8pYUVfYtlZWh02zkNIX0FAEDu9G9ib1Q4c88z0EkFic3m4XZ8ToyNLw/640?wx_fmt=png&from=appmsg&tp=wxpic&wxfrom=5&wx_lazy=1&wx_co=1)

---

![图片](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/Ollama.png)

---

## 三、硅基流动注册

[官网注册地址](https://cloud.siliconflow.cn/i/hsszzZrZ)

![Siliconflow注册](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/Siliconflow注册.png)

注册登录后，直接在左侧菜单栏找到`模型广场`，其中列出的模型都是硅基流动支持的，有些是**免费**的，有些需要**消耗平台余额**。比如满血版`DeepSeek-R1`就会消耗余额，而DeepSeek的文生图模型`Janus-Pro-7B`则是完全免费的。

![模型广场](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/model%E9%80%89%E6%8B%A9.png)

---

以满血版`DeepSeek-R1`为例，点击**模型卡片**，可以**查看模型的详情**。可以看到，`DeepSeek-R1`是一个MoE混合专家模型，满血版参数为671B，上下文长度为64K tokens。

![DeepSeekR1](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/DeepSeekR1.png)

---

### **1. 模型配置**

安装好Cherry Studio后，首先需要**配置模型**。搭建个人知识库，仅硅基流动的模型就足够了，所以只需：点击左下角的`设置`->选择`硅基流动`->打开开关->填入硅基流动的API key。

![CherryStudio配置](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/CherryStudio%E9%85%8D%E7%BD%AE.png)

---

接下来，把需要用到的**模型添加**上。下拉到最下面，点击绿色的`管理`按钮。

![模型管理2](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%862.png)

---

常规模型中，推荐添加下面三个模型：**推理模型**`DeepSeek-R1`、**通用模型**`DeepSeek-V3`，以及**视觉模型**`Janus-Pro-7B`。

![配置DeepSeek模型](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/%E9%85%8D%E7%BD%AEDeepSeek%E6%A8%A1%E5%9E%8B.png)

---

嵌入（embedding）模型，推荐添加下面两个：**完全免费**的`BAAI/bge-m3`和**付费**的`Pro/BAAI/bge-m3`。

![嵌入embedding模型](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/%E5%B5%8C%E5%85%A5embedding%E6%A8%A1%E5%9E%8B.png)

一般说来，免费的`BAAI/bge-m3`模型就够用。

---

### **2. 创建知识库**

> 配置完模型，就可以开始**创建知识库**了！

点击左侧菜单栏的`知识库`图标->点击左上角的`添加`按钮->在弹窗里输入你的知识库名称，随便输，方便查找就行->选择`嵌入模型`。

嵌入模型选择上一步中添加的免费模型`BAAI/bge-m3`就可以。

![create知识库](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/create%E7%9F%A5%E8%AF%86%E5%BA%93.png)

---

接下来就可以往你创建好的知识库里**添加资料**，Cherry Studio支持各种类型的资料，比如文件、网址、笔记等等。

![add知识库资料1](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/add%E7%9F%A5%E8%AF%86%E5%BA%93%E8%B5%84%E6%96%991.png)

---

以最常见的文件资料为例，直接把PDF**拖拽**进去，当看到文件右边的状态符号变为绿色的对勾，就说明该文件已经**向量化**完毕。

![add知识库资料2](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/add%E7%9F%A5%E8%AF%86%E5%BA%93%E8%B5%84%E6%96%992.png)

---

### **3. 和知识库对话**

添加完资料，就可以开始**检索**你的个人知识库了。

两种方式使用。

> 一种是直接在知识库最下面的`搜索知识库`，点击后进行搜索。

输入你想搜索的内容，点击`搜索`按钮。就像下面这样。

![search知识库](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/search%E7%9F%A5%E8%AF%86%E5%BA%93.png)

---

> 第二种方法则更为实用：直接在问答的过程中**选中知识库**，相当于给LLM添加了额外的上下文信息。

在Cherry Studio的输入框下方，有一个知识库的图标，点击，**选择**你创建好的知识库。

![选择知识库](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/%E9%80%89%E6%8B%A9%E7%9F%A5%E8%AF%86%E5%BA%93.png)

---

选中后，知识库的图标会变成**蓝色**。

![选择知识库2](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/%E9%80%89%E6%8B%A9%E7%9F%A5%E8%AF%86%E5%BA%932.png)

---

这里我默认的模型是`DeepSeek-R1`。回答前，会先对知识库进行检索，然后把搜索结果投喂给`DeepSeek-R1`，由模型进行整理、分析，再生成最终的答案。

可以看到，`DeepSeek-R1`的回答结果是基于**知识库内容**产生的。

![基于知识库回复](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/%E5%9F%BA%E4%BA%8E%E7%9F%A5%E8%AF%86%E5%BA%93%E5%9B%9E%E5%A4%8D.png)

---

`DeepSeek-R1`的思考过程。

![DeepSeek思考过程](https://raw.githubusercontents.com/DongMing0103/MarkdownCloudImage/master/data/work/DeepSeek%E6%80%9D%E8%80%83%E8%BF%87%E7%A8%8B.png)

------

 
